{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to alter the values of the \"constants\" I have declared in lines 10-12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/anerud001/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/anerud001/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "FILE_MATCHES = 1  #Only want the  top one\n",
    "SENTENCE_MATCHES = 1  #Only want the top one\n",
    "CORPUS= 'testCorpus' #for testing provided directories: one, testCorpus, corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(directory):\n",
    "    \"\"\"\n",
    "    Given a directory name, return a dictionary mapping the filename of each\n",
    "    `.txt` file inside that directory to the file's contents as a string.\n",
    "    \"\"\"\n",
    "    corpus = dict()\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith(\".txt\"):\n",
    "            with open(file_path, \"r\", encoding='utf8') as file:\n",
    "                corpus[filename] = file.read()\n",
    "    #feel free to print the corpus on small corpus though - I initally set CORPUS to one - so it is small\n",
    "    #you should do this so you know what you are working with\n",
    "    #print(corpus)#testing ONLY print this then comment it out\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(document):\n",
    "    \"\"\"\n",
    "    Given a document (represented as a string), return a list of all of the\n",
    "    words in that document, in order.\n",
    "\n",
    "    Process document by coverting all words to lowercase, and removing any\n",
    "    punctuation or English stopwords.\n",
    "    \"\"\"\n",
    "    #print(document)\n",
    "    exclude = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    words = nltk.word_tokenize(document)#get every word in document in a list\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        word = word.lower()#all words to lowercase\n",
    "        word = ps.stem(word)#stemming\n",
    "        word = ''.join(ch for ch in word if ch not in exclude)#this removes punctuation\n",
    "        if word not in stop_words and len(word)>0:# if it is not a stopword and it is not empty add it to the list\n",
    "            clean_words.append(word)\n",
    "    sorted_words = sorted(clean_words)\n",
    "    #feel free to print the corpus on small corpus though - I initally set CORPUS to one - so it is small\n",
    "    #you should do this so you know what you are working with\n",
    "    #print(sorted_words)#testing ONLY print this then comment it out\n",
    "    return sorted_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idfs(documents):\n",
    "    \"\"\"\n",
    "    Given a dictionary of `documents` that maps names of documents to a list\n",
    "    of words, return a dictionary that maps words to their IDF values.\n",
    "\n",
    "    Any word that appears in at least one of the documents should be in the\n",
    "    resulting dictionary.\n",
    "    \"\"\"\n",
    "    # create an empty dictionary that will hold key word: document using word count\n",
    "    counts = dict()\n",
    "    #for each of the files (key) in documents(dictionary parameter you are given)\n",
    "    for filename in documents:\n",
    "        seen_words = set()\n",
    "        for word in documents[filename]:\n",
    "            if word not in seen_words:\n",
    "                seen_words.add(word)\n",
    "                try:\n",
    "                    counts[word] += 1\n",
    "                except KeyError:\n",
    "                    counts[word] = 1\n",
    "                    \n",
    "    return {word: math.log(1+ (len(documents)/counts[word])) for word in counts}\n",
    "        \n",
    "        #create something that can store unique words\n",
    "        #for each word in the string that is stored in the dictionary entry\n",
    "            #if you have not seen the word already\n",
    "                #add it to your unique word store\n",
    "                #in your dictionary increment the number count for the number of documents using the word\n",
    "                #if there is not the dictionary yet - make an enter with count of documents set to 1\n",
    "    # You need to create a dictionary to hold key word: idf that you will return\n",
    "    # I think it is easier to use a dictionary comprehension once you have the dictionary with the word count finished\n",
    "    # for every word in dictionary of number of documents using word count\n",
    "        #make an entry in dictionary entry using the word as the key and the calculated idf has the entry\n",
    "    #return the idf dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_documents(query, files, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `files` (a dictionary mapping names of\n",
    "    files to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the filenames of the the `n` top\n",
    "    files that match the query, ranked according to tf-idf.\n",
    "    \"\"\"\n",
    "    #create an empty dictionary\n",
    "    tf_idfs = dict()\n",
    "    #for every entry in the files dictionary\n",
    "    for filename in files:\n",
    "        #make an entry into the empty dictionary with a key of the filename with an entry set to zero\n",
    "        tf_idfs[filename] = 0\n",
    "        #for every word in the query\n",
    "        for word in query:\n",
    "            #find the tf for the word and the idf for the word\n",
    "            #accumulate the tf*idf for the entry for this file\n",
    "            tf_idfs[filename] += files[filename].count(word)*idfs[word]\n",
    "    #sort the dictionary for the highest tf*idf value\n",
    "    #return a list of the top n files\n",
    "    return[key for key, value in sorted(tf_idfs.items(), key=lambda item:item[1], reverse=True)][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_sentences(query, sentences, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `sentences` (a dictionary mapping\n",
    "    sentences to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the `n` top sentences that match\n",
    "    the query, ranked according to idf. If there are ties, preference should\n",
    "    be given to sentences that have a higher query term density.\n",
    "    \"\"\"\n",
    "    #create an empty list to hold the best sentences\n",
    "    best_sentences = list()\n",
    "    #for every sentence in the list of sentences passed\n",
    "    for sentence in sentences:\n",
    "        #you need to keep track of two bits of information the cumulative idfs and term density for ties for each sentence\n",
    "        sentence_values = [sentence, 0, 0]\n",
    "        #for every word in the query\n",
    "        for word in query:\n",
    "            #if the word is in the sentence\n",
    "            if word in sentence:\n",
    "                #accumulate the idfs index 1\n",
    "                sentence_values[1] += idfs[word]\n",
    "                #accumulate term density idx 2\n",
    "                sentence_values[2] += sentence.count(word)/len(sentence)\n",
    "        #store the idfs and term density for the sentence\n",
    "        best_sentences.append(sentence_values)\n",
    "    #sort based on idf, if tie - use term density\n",
    "    #return a list of the top n sentences\n",
    "    return [sentence for sentence, idf, den in sorted(best_sentences, key=lambda item:(item[1], item[2]),reverse=True)][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Calculate IDF values across files\n",
    "    files = load_files(CORPUS)#uses function from above\n",
    "    file_words = {\n",
    "        filename: process(files[filename])#uses process function from above\n",
    "        for filename in files\n",
    "    }\n",
    "    #for you to see what file_words is but comment out when you have seen what it is\n",
    "    #print(file_words)\n",
    "    \n",
    "    #uncomment when ready to test\n",
    "    \n",
    "    #this uses your function\n",
    "    file_idfs = calculate_idfs(file_words)\n",
    "    \n",
    "    \n",
    "    # Prompt user for query\n",
    "    query = set(process(input(\"Query: \")))\n",
    "    \n",
    "    try:\n",
    "        # Determine top file matches according to TF-IDF\n",
    "        filenames = relevant_documents(query, file_words, file_idfs, n=FILE_MATCHES)#Can alter the n by altering constant\n",
    "    except KeyError:\n",
    "        print(\"We cannot help for that request\")\n",
    "        return\n",
    "        \n",
    "    # Extract sentences from top files\n",
    "    sentences = dict()\n",
    "    for filename in filenames:\n",
    "        for passage in files[filename].split(\"\\n\"):\n",
    "            for sentence in nltk.sent_tokenize(passage):\n",
    "                tokens = process(sentence)#use function from above\n",
    "                if tokens: #is tokens is not empty\n",
    "                    sentences[sentence] = tokens\n",
    "\n",
    "    # Compute IDF values across sentences\n",
    "    idfs = calculate_idfs(sentences)\n",
    "\n",
    "    # Determine top sentence matches\n",
    "    matches = relevant_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)#can alter the n\n",
    "    for match in matches:\n",
    "        print(match)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start to code - run this using the CORPUS one and then with testCorpus so you understand the output that will be used as input. The comment out the print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: how is scooby-doo\n",
      "Scooby-Doo is an American animated franchise comprising many animated television series produced from 1969 to the present, as well as their derivative media.\n"
     ]
    }
   ],
   "source": [
    "#comment out when you want to run\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
